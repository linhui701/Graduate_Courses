\documentclass[12pt,thmsa]{article}

\usepackage{amsmath}
\usepackage{amsthm}    % for proof
\usepackage{amsfonts}  % for \mathbb
\usepackage{mathrsfs}  % for Ralph Smith's Formal Script Font
\usepackage[mathscr]{euscript} %redefine the \mathcal command to use Euler script 
\usepackage{algorithm} % http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{graphicx}  % Allows including images
\usepackage{booktabs}  % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{multirow}
\usepackage{multicol}
\usepackage{cancel}
\usepackage{color}     % color
\usepackage{geometry}
\usepackage{amssymb}   % \varnothing, \bigstar, \blacksquare, \clubsuit, \blacktriangleright, \diamondsuit, \spadesuit, \dagger, \checkmark
\usepackage{textcase}  % \MakeTextUppercase
\usepackage{float}     % Force figure placement in text with [H]
\usepackage[export]{adjustbox}
\usepackage{enumitem}
\setlist[itemize]{leftmargin=*} % global option, remove the indentation for a specific list
\renewcommand{\qedsymbol}{$\blacksquare$} % change the QED symbol to a filled square
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows.meta, positioning}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}

\graphicspath{ {./Figure/} }
\geometry{
	a4paper,
	total={170mm,257mm},
	left=20mm,
	right=20mm,
	top=20mm,
}

% Linhui added for newly defined color
\definecolor{forestgreen}{RGB}{34,139,34}

% Linhui added for Expectation and Variance
\newcommand{\Exp}{{\mathbb E}\! }
\newcommand{\Var}{\mbox{Var}\! }

% Linhui added for rename the command for empty set.
\let\oldemptyset\emptyset
\let\emptyset\varnothing

% Linhui added for empty box.
\newcommand{\emptybox}[2][0.6em]{% 
	\fbox{\rule{0pt}{#1}\hspace{#2}}%
}

%------------------------------------------------------%
\makeatletter
\def\maketitle{%
	\par
	\hrule height 1.5pt\vspace{1ex}
	\par\noindent
	
	\begin{minipage}{0.5\textwidth}
		\scshape
		Purdue University \(\cdot\) ece 58000 \\[1ex]
		Optimization Methods \\
		Prof. Zak, Prof. Chong
	\end{minipage}
	\begin{minipage}{0.45\textwidth}
		\raggedleft
		\MakeTextUppercase{{\@title}}\\[0.3ex] % 0.2ex height space between two line
		\textit{\@author}\\[0.2ex]
		\textit{September 1, 2022}
	\end{minipage}
	\par\vspace{1ex}
	\hrule height 1.5pt\vspace{1ex}
	\par
}
\makeatother

\author{Linhui Xie}
\title{Lecture Note 02}
%------------------------------------------------------%

\begin{document}
\maketitle

\setcounter{section}{1}
\section{Convexity, Derivative\medskip}

\setcounter{section}{2}

%------------------------------------------------------%
\subsection{Lines, hyperplanes and linear varieties}

\begin{itemize}
	\item The \underline{line segment} between two points \(\boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^{n}\) is the set,
	\[
	\{{\color{red}{\boldsymbol{z}}} \in \mathbb{R}^{n}: \boldsymbol{z} = \alpha \boldsymbol{x}+(1-\alpha) \boldsymbol{y}, \alpha \in[0,1]\}.
	\]
	
	\item A hyperplane of the space \(\mathbb{R}^{n} \), is the set of all points \(\boldsymbol{x}=\left[x_{1}, x_{2}, \ldots, x_{n}\right]^{\top}\) that satisfy the linear equation
	\[
	u_{1} x_{1}+u_{2} x_{2}+\cdots+u_{n} x_{n}=v,
	\]
	where at least one of the \(u_{i}\) is nonzero. The \underline{hyperplane}  is defined by
	\[
	\left\{{\color{red}{\boldsymbol{x}}} \in \mathbb{R}^{n}: \boldsymbol{u}^{\top} \boldsymbol{x}=v\right\},
	\]where
	\[
	\boldsymbol{u}=\left[u_{1}, u_{2}, \ldots, u_{n}\right]^{\top}.
	\]
	
	\item Two \underline{half-spaces}, postive half-space and negative half-space are
	\[ H_{+}=\left\{{\color{red}{\boldsymbol{x}}} \in \mathbb{R}^{n}: \boldsymbol{u}^{\top} \boldsymbol{x} \geq v\right\}, \]
	\[ H_{-}=\left\{{\color{red}{\boldsymbol{x}}} \in \mathbb{R}^{n}: \boldsymbol{u}^{\top} \boldsymbol{x} \leq v\right\}. \]
	
	\item  A \underline{linear variety} is a set of form
	\[ \left\{\boldsymbol{x} \in \mathbb{R}^{n} : \boldsymbol{A}\boldsymbol{x} = \boldsymbol{b} \right\},  \]
	for some matrix \(\boldsymbol{A} \in \mathbb{R}^{m \times n}\) and vector \(\boldsymbol{b} \in \mathbb{R}^{n}\).
\end{itemize}

%------------------------------------------------------%
\subsection{Convex sets}

\begin{itemize}
	\item A point \(\boldsymbol{w}=\alpha \boldsymbol{u}+(1-\alpha) \boldsymbol{v}\) (where \(\alpha \in[0,1]\)) is called a \underline{convex combination} of the points \(\boldsymbol{u}\) and \(\boldsymbol{u}\).


	\item A set \(\Theta \subset \mathbb{R}^{n}\) is \underline{convex} if for all \( \boldsymbol{u}, \boldsymbol{v} \in \Theta \), the \textit{line segment} between \(\boldsymbol{u}\) and \(\boldsymbol{v}\) is in \(\Theta \). 
	
	That is, \(\Theta\) is \textit{convex} if and only if \(\alpha \boldsymbol{u}+(1-\alpha) \boldsymbol{v} \in \Theta \) for all \(\boldsymbol{u}, \boldsymbol{v} \in \Theta\) and \(\alpha \in(0,1)\). Examples of convex sets include the following:
	\begin{multicols}{2}
		\begin{itemize}
			\item The empty set
			\item A set consisting of a single point
			\item A line or a line segment
			\item A subspace
			\item A hyperplane
			\item A linear variety
			\item A half-space
			\item \(\mathbb{R}^{n}\)
		\end{itemize}
	\end{multicols}

	\item[\(\spadesuit\)] \(\mathscr{THEOREM}\)4.3 Convex subsets of \(\mathbb{R}^{n}\) have the following properties:
	
	\begin{itemize}
		\item[a.] If \(\Theta\) is a \textit{convex set} and \(\beta\) is a real number, then the set
		\[
		\beta \Theta=\{\boldsymbol{x}: \boldsymbol{x}=\beta \boldsymbol{v}, \boldsymbol{v} \in \Theta\}
		\]is also convex.
		
		\item[b.] If \(\Theta_{1}\) and \(\Theta_{2}\) are \textit{convex sets}, then the set
		\[
		\Theta_{1}+\Theta_{2}=\left\{\boldsymbol{x}: \boldsymbol{x}=\boldsymbol{v}_{1}+\boldsymbol{v}_{2}, \boldsymbol{v}_{1} \in \Theta_{1}, \boldsymbol{v}_{2} \in \Theta_{2}\right\}
		\]is also convex.
		
		\item[c.] The intersection of any collection of \textit{convex sets} is convex.
	
	\end{itemize}
	% Convexity Prove that that if \(\Theta\) and \(\Sigma\) are convex sets and \(a\) and \(b\) are real numbers, then \(a \Theta+b \Sigma\) is convex.
	
	% Convexity of intersections Prove that that if \(\Theta\) and \(\Sigma\) are convex sets and \(a\) and \(b\) are real numbers, then \( a \Theta \cap b \Sigma \) is convex.
	
	\item An \underline{extreme point} \(\boldsymbol{x}\) in a \textit{convex set} \(\Theta\), if there are no two distinct points \(\boldsymbol{u}\) and \(\boldsymbol{v}\) in \(\Theta\) such that \(\boldsymbol{x}=\alpha \boldsymbol{u}+(1-\alpha) \boldsymbol{v}\) for some \(\alpha \in(0,1)\).
	
\end{itemize}


%------------------------------------------------------%
\subsection{Differentiation rules}
\begin{itemize}
	\item A function \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\) follows,
	\[ 
	\begin{aligned}
		f(\boldsymbol{x})
		=f \left(\left[ 
		\begin{array}{c}{x_{1}} \\ {x_{2}} \\ {\vdots} \\ {x_{n}}\end{array} 
		\right] \right)
		&=a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n}
		=\left[ 
		\begin{array}{lll}{a_{1}} & {\cdots} & {a_{n}}\end{array} 
		\right]
		\left[ 
		\begin{array}{c}{x_{1}} \\ {\vdots} \\ {x_{n}}\end{array} 
		\right] \\
		& =\left[
		\begin{array}{ccc}{} & {\boldsymbol{a}^{\top}} & {}\end{array}
		\right]
		\left[
		\begin{array}{c}{\vdots} \\ { \boldsymbol{x}} \\ {\vdots} \end{array}
		\right] 
		=\left[
		\begin{array}{ccc}{} &  {\boldsymbol{x}^{\top}} & {}\end{array}
		\right]
		\left[
		\begin{array}{c}{\vdots} \\ { \boldsymbol{a}} \\ {\vdots} \end{array}
		\right]. 
	\end{aligned}
	\]
	
	\item A matrix \(\mathbf{A} \in \mathbb{R}^{\color{black}{m} \times \color{black}{n}} \), \( \mathbf{A} \) is \(m \times n\) matrix,
	\[ \mathbf{A}
	= \left[
	\begin{array}{cccc} 
		{\vdots} & {\vdots} & & {\vdots} \\ 
		{\boldsymbol{a}_{{\color{forestgreen}*}1}} & {\boldsymbol{a}_{{\color{forestgreen}*}2}} & {\cdots} & {\boldsymbol{a}_{{\color{forestgreen}*}n}}\\ 
		{\vdots} & {\vdots} & & {\vdots}
	\end{array}
	\right] 
	= \left[
	\begin{array}{cccc}
		a_{{\color{forestgreen}1}1} & a_{{\color{forestgreen}1}2} & \cdots & a_{{\color{forestgreen}1}n} \\
		a_{{\color{forestgreen}2}1} & a_{{\color{forestgreen}2}2} & \cdots & a_{{\color{forestgreen}2}n} \\
		\vdots & \vdots & \ddots & \vdots \\
		a_{{\color{forestgreen}m}1} & a_{{\color{forestgreen}{\color{forestgreen}1}}2} & \cdots & a_{{\color{forestgreen}m}n} \\
	\end{array}
	\right]
	=\left[
	\begin{array}{c}{\boldsymbol{a}_{{\color{forestgreen}1}}^{\top}} \\
		\boldsymbol{a}_{{\color{forestgreen}2}}^{\top} \\
		{\vdots} \\ 
		{\boldsymbol{a}_{{\color{forestgreen}m}}^{\top}}
	\end{array}
	\right].
	\]
	
	\item A function \(\boldsymbol{g}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\) and a matrix \(\mathbf{A} \in \mathbb{R}^{\color{black}{m} \times \color{black}{n}} \), \( \mathbf{A} \boldsymbol{x} \) is a column vector whose element is a scalar \(g_{\star}(\boldsymbol{x})\). 
	\[ \mathbf{A} \boldsymbol{x}
	=\left[
	\begin{array}{c}{\boldsymbol{a}_{1}^{\top}} \\ {\vdots} \\ {\boldsymbol{a}_{m}^{\top}}\end{array}
	\right] \boldsymbol{x}
	=\left[
	\begin{array}{c}{
			\boldsymbol{a}_{1}^{\top} \boldsymbol{x}} \\ 
		{\vdots} \\ 
		{\boldsymbol{a}_{m}^{\top}\boldsymbol{x}}
	\end{array}
	\right]
	=\left[
	\begin{array}{c}{
			a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}} \\ 
		{\vdots} \\ 
		{a_{m 1} x_{1}+a_{m 2} x_{2}+\cdots+a_{m n} x_{n}}
	\end{array}
	\right]
	=\left[
	\begin{array}{c}{
			g_{1}(\boldsymbol{x})} \\ 
		{\vdots} \\ 
		{g_{m}(\boldsymbol{x})}
	\end{array}
	\right]
	= \boldsymbol{g}(\boldsymbol{x}).
	\]

	\item To be noted, in this course, we write the \textbf{derivative} \(D( f(\boldsymbol{x})  ) \) as a {\color{red}{row vector}}, and write the \textbf{gradient} \( \nabla f(\boldsymbol{x}) \) as a {\color{blue}{column vector}}.
		
\end{itemize}

\newpage
\[\text{Types of Matrix Derivatives}\]
\[
\begin{aligned}
	\begin{array}{|c|c|c|c|}
		% \hline
		% \multirow{2}{*}{\text{Types}} & \multicolumn{1}{c|}{\text{Scalar}} & \multicolumn{1}{c|}{\text{Vector}} & \multicolumn{1}{c|}{\text{Matrix}} \\
		% \cline{2-4} & \emptybox{0.8em} & \emptybox{0.8em} & \emptybox{0.8em} \\
		\hline
		\multirow{2}{*}{ \text { Types }} & \text { Scalar } & \text { Vector } & \text { Matrix } \\
		\cline{2-4} & \fbox{\vspace{1em}\hspace{1em}} & \fbox{\vspace{1em}\hspace{1em}} & \fbox{\vspace{1em}\hspace{1em}} \\
		\hline
		\fbox{\vspace{1em}\hspace{1em}}
		 & \multirow{2}{*}{ \(\frac{d f}{d x}\), \(\frac{\partial f}{\partial x_{*}}\) \hspace{0.8cm} (1)} & \multirow{2}{*}{ \(\frac{d  \boldsymbol{g}(t)}{d t}\), \(\frac{\partial  \boldsymbol{g}(\boldsymbol{x})}{\partial x_{*}}\) (3)} & \multirow{2}{*}{ \(\frac{d  \boldsymbol{A}(t)}{d t}\) } \\
		\cline{1-1} \text{Scalar} &  &  &  \\
		\hline
		\fbox{\vspace{1em}\hspace{1em}}
		& \multirow{2}{*}{ \(\frac{\partial f(\boldsymbol{x}) }{\partial  \boldsymbol{x}}\), \(\nabla f(\boldsymbol{x})\) (2) } & \multirow{2}{*}{ \(\frac{\partial  \boldsymbol{g}(\boldsymbol{x}) }{\partial  \boldsymbol{x}}\) \hspace{0.9cm} (4)} & {} \\
		\cline{1-1} \text{Vector} &  &  &  \\
		\hline
		\fbox{\vspace{1em}\hspace{1em}}
		& \multirow{2}{*}{ \(\frac{\partial f}{\partial  \mathbf{X}}\) } & {} & {} \\
		\cline{1-1} \text{Matrix} &  &  &  \\
		\hline
	\end{array}
\end{aligned}
\]

\begin{itemize}
	\item[({\bf{1}})] Given \(f: \mathbb{R} \rightarrow \mathbb{R}\), the derivative of \(f\) is a function \(f^{\prime}: \mathbb{R} \rightarrow \mathbb{R}\) given by
	\[
	D_{x}(f(x)) = \frac{d f}{d x} = f^{\prime}(x)=\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h},
	\]
	if the limit exists.
	
	\item[({\bf{2}})] Given \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\), consider a scalar \( f(\boldsymbol{x}) = a_{1} x_{1}+a_{2} x_{2}+\cdots+a_{n} x_{n} =  \boldsymbol{a}^{\top} \boldsymbol{x}\).

	For \textbf{derivative} rule (2), 
	\[ {\color{black}{
			\frac{\partial}{\partial \boldsymbol{x}}}} f( \boldsymbol{x} ) 
	={\color{red}{D_{\boldsymbol{x}}(}}{ f(\boldsymbol{x}) }{\color{red})}
	={\color{red}{D_{\boldsymbol{x}}(}}{ \boldsymbol{a}^{\top} \boldsymbol{x} }{\color{red})} =
	\left[\begin{array}{cccc}
		\frac{\partial f}{\partial x_{1}} & 
		\frac{\partial f}{\partial x_{2}} & 
		\cdots & 
		\frac{\partial f}{\partial x_{n}}
	\end{array}\right]
	=\left[\begin{array}{ccc}
		a_{1} & \cdots & a_{n}
	\end{array}
	\right] = {\color{red}{ \boldsymbol{a}^{\top} }},
	\]

	For \textbf{gradient} rule (2), \underline{if \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\) is differentiable}, then the \textit{gradient} of \(f\) is a function \(\nabla f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}\) given by
	\[  {\color{blue}{\nabla}_{\boldsymbol{x}} } f(\boldsymbol{x}) =
	\left[\begin{aligned}
		&\frac{\partial f}{\partial x_{1}} \\ 
		&\frac{\partial f}{\partial x_{2}} \\  
		&\vdots \\  
		&\frac{\partial f}{\partial x_{n}}  
	\end{aligned}\right]
	= \left[ \begin{array}{c} a_1 \\ a_2 \\ \vdots \\ \vdots \\ a_n \end{array} \right]
	= {\color{blue}{ \boldsymbol{a} }}
	= D_{\boldsymbol{x}}( f(\boldsymbol{x})  )^{\top},
	\] 
%	\[ {\color{blue}{\nabla}} ( \mathbf{A} \boldsymbol{x} ) 
%	=\left[
%	\begin{array}{c}{
%			\nabla \boldsymbol{a}_{1}^{\top} \boldsymbol{x}} \\ 
%		{\vdots} \\ 
%		\nabla {\boldsymbol{a}_{m}^{\top}\boldsymbol{x}}
%	\end{array}
%	\right] 
%	=\left[
%	\begin{array}{c}{
%			\nabla f_{1}(\boldsymbol{x})} \\ 
%		{\vdots} \\ 
%		\nabla {f_{m}(\boldsymbol{x})}
%	\end{array}
%	\right] 
%	={\color{blue}{ \mathbf{A}^{\top} }} = D( {\color{black}{  \mathbf{A} \boldsymbol{x}  }} )^{\top}.    \]

	\item[({\bf{3}})] Given \(\boldsymbol{g}: \mathbb{R} \rightarrow \mathbb{R}^m \), here \(t \in \mathbb{R} \) is a scalar. \(\boldsymbol{g}(t)\) is a column vector.
	\[
	\begin{aligned}
		\boldsymbol{g}(t) =
		\left[
		\begin{array}{c}
			g_{1}(t) \\
			g_{2}(t) \\
			\vdots \\
			g_{m}(t)
		\end{array}
		\right], \quad
		D_{t} \boldsymbol{g}(t) & =\left[
		\begin{aligned}
			\frac{d}{d t}g_{1}(t) \\
			\frac{d}{d t}g_{2}(t) \\
			\vdots \qquad \\
			\frac{d}{d t}g_{m}(t)
		\end{aligned}
		\right] =\left[\begin{array}{c}
			g_{1}^{\prime}(t) \\
			\vdots \\
			g_{m}^{\prime}(t)
		\end{array}\right],  \\
		% \nabla \boldsymbol{g}(t) & =\Bigg[
		% \begin{aligned}
		% 	\frac{d}{d t}g_{1}(t) \quad 
		% 	\frac{d}{d t}g_{2}(t) \quad
		% 	\cdots  \quad
		% 	\frac{d}{d t}g_{m}(t)
		% \end{aligned}
		% \Bigg] =\bigg[ \begin{array}{ccc}
		% 	g_{1}^{\prime}(t) &
		% 	\cdots &
		%	g_{m}^{\prime}(t)
		%\end{array} \bigg].  \\
%		\nabla \boldsymbol{g}(t) & =\left[
%		\begin{aligned}
%			\frac{d}{d t}g_{1}(t) \\
%			\frac{d}{d t}g_{2}(t) \\
%			\vdots \qquad \\
%			\frac{d}{d t}g_{m}(t)
%		\end{aligned}
%		\right] =\left[\begin{array}{c}
%			g_{1}^{\prime}(t) \\
%			\vdots \\
%			g_{m}^{\prime}(t)
%		\end{array}\right].  \\
	\end{aligned}
	\]


	\item[({\bf{4}})] Consider \(\boldsymbol{g}: \mathbb{R}^{n} \rightarrow \mathbb{R}^m \),  here \( \boldsymbol{x} \in  \mathbb{R}^n \) is a vector. Since \(g_{i}(\boldsymbol{x})\) is a scalar, \(\boldsymbol{g}=\left[g_1, \ldots, g_m\right]^{\top} \), \(\boldsymbol{g}(\boldsymbol{x})\) is a column vector.
	\[
	\begin{aligned}
		\boldsymbol{g} \left( \boldsymbol{x} \right)=
		\left[
		\begin{array}{c}
			g_{1}( \boldsymbol{x} ) \\
			g_{2}( \boldsymbol{x} ) \\
			\vdots \\
			g_{m}( \boldsymbol{x} )
		\end{array}
		\right], 
		D_{\boldsymbol{x}} \boldsymbol{g}\left( \boldsymbol{x} \right) & =\left[
		\begin{aligned}
			D_{\boldsymbol{x}} g_{1}\left( x_1,x_2,\cdots,x_n \right) \\
			D_{\boldsymbol{x}} g_{2}\left( x_1,x_2,\cdots,x_n \right) \\
			\vdots \qquad \qquad \\
			D_{\boldsymbol{x}} g_{m}\left( x_1,x_2,\cdots,x_n \right)
		\end{aligned}
		\right]
		= \left[
		\begin{array}{cccc}
			\frac{\partial}{\partial x_1} g_1
			& \frac{\partial}{\partial x_2} g_1
			& \cdots 
			& \frac{\partial}{\partial x_n} g_1 \\
			\frac{\partial}{\partial x_1} g_2
			& \frac{\partial}{\partial x_2} g_2
			& \cdots 
			& \frac{\partial}{\partial x_n} g_2 \\
			\vdots & \vdots & \ddots & \vdots \\
			\frac{\partial}{\partial x_1} g_m
			& \frac{\partial}{\partial x_2} g_m
			& \cdots 
			& \frac{\partial}{\partial x_n} g_m \\
		\end{array}
		\right] = \boldsymbol{L}. \\
%		\nabla \boldsymbol{g}\left( \boldsymbol{x} \right) & =\Bigg[
%		\begin{aligned}
%			\nabla g_{1}\left( \boldsymbol{x} \right) \ 
%			\cdots \ 
%			\nabla g_{m}\left( \boldsymbol{x} \right) 
%		\end{aligned}
%		\Bigg]
%		= \left[
%		\begin{array}{cccc}
%			\frac{\partial}{\partial x_1} g_1
%			& \frac{\partial}{\partial x_1} g_2
%			& \cdots 
%			& \frac{\partial}{\partial x_1} g_m \\
%			\frac{\partial}{\partial x_2} g_1
%			& \frac{\partial}{\partial x_2} g_2
%			& \cdots 
%			& \frac{\partial}{\partial x_2} g_m \\
%			\vdots & \vdots & \ddots & \vdots \\
%			\frac{\partial}{\partial x_n} g_1
%			& \frac{\partial}{\partial x_n} g_2
%			& \cdots 
%			& \frac{\partial}{\partial x_n} g_m \\
%		\end{array}
%		\right] = \boldsymbol{L}^{\top}. \\
	\end{aligned}
	\]
	The matrix \(\boldsymbol{L}\) is called the Jacobian matrix, or derivative matrix, of function \(\boldsymbol{g}\).
\end{itemize}
	
\newpage
	
\begin{itemize}
	\item If all elements in \(\boldsymbol{g}( \boldsymbol{x} )\) are linear combination of \(\boldsymbol{x}\),
	\[ \boldsymbol{g}( \boldsymbol{x} )
	=\left[
	\begin{array}{c}{
			g_{1}(\boldsymbol{x})} \\ 
		{\vdots} \\ 
		{g_{m}(\boldsymbol{x})}
	\end{array}
	\right]
	=\left[
	\begin{array}{c}{
			a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}} \\ 
		{\vdots} \\ 
		{a_{m 1} x_{1}+a_{m 2} x_{2}+\cdots+a_{m n} x_{n}}
	\end{array}
	\right]
	=\left[
	\begin{array}{c}{
			\boldsymbol{a}_{1}^{\top} \boldsymbol{x}} \\ 
		{\vdots} \\ 
		{\boldsymbol{a}_{m}^{\top}\boldsymbol{x}}
	\end{array}
	\right]
	=\left[
	\begin{array}{c}{\boldsymbol{a}_{1}^{\top}} \\ {\vdots} \\ {\boldsymbol{a}_{m}^{\top}}\end{array}
	\right] \boldsymbol{x}
	= \mathbf{A} \boldsymbol{x}.
	\]
	
	Then, the derivative of \(\mathbf{A} \boldsymbol{x}\) is equivalent to \(\frac{\partial}{\partial \boldsymbol{x}} \boldsymbol{g}( \boldsymbol{x} ) \),
	\[ \frac{\partial}{\partial \boldsymbol{x}} \boldsymbol{g}( \boldsymbol{x} )
	= \frac{\partial}{\partial \boldsymbol{x}} \left(  \mathbf{A} \boldsymbol{x} \right)
	= {\color{red}{ D_{\boldsymbol{x}}( }} \mathbf{A} \boldsymbol{x} {\color{red}{ ) }} 
	= \left[
		\begin{array}{c}
			{\color{red}{D_{\boldsymbol{x}}(}}{ \boldsymbol{a}_{1}^{\top} \boldsymbol{x} }{\color{red})} \\
			{\color{red}{D_{\boldsymbol{x}}(}}{ \boldsymbol{a}_{2}^{\top} \boldsymbol{x} }{\color{red})} \\
			\vdots \\
			{\color{red}{D_{\boldsymbol{x}}(}}{ \boldsymbol{a}_{m}^{\top} \boldsymbol{x} }{\color{red})}
		\end{array}
		\right]
	= \left[
		\begin{array}{c}
			\boldsymbol{a}_{1}^{\top}\\
			\boldsymbol{a}_{2}^{\top}\\
			\vdots \\
			\boldsymbol{a}_{m}^{\top}
		\end{array}
		\right]
	= \mathbf{A}.
	\]
	
	\item In summary, the derivative rules are listed as,
	% align automatically places the equations in math mode 
	\begin{align*}
		{\color{red} D(} \boldsymbol{a}^{\top} \boldsymbol{x} {\color{red})} &= \boldsymbol{a}^{\top}, \qquad &
		\fbox{\parbox{0.36\textwidth}{\( (2) f: \mathbb{R}^{n} \rightarrow \mathbb{R}, f(\boldsymbol{x}) =  \boldsymbol{a}^{\top} \boldsymbol{x} \)}} \\
		{\color{red} D(} \boldsymbol{g}(t) {\color{red})} &=
		\left[ \begin{array}{c} \vdots \\ g_{*}'(t) \\ \vdots \end{array} \right], \qquad &
		\fbox{\parbox{0.36\textwidth}{\( (3) \boldsymbol{g}: \mathbb{R} \rightarrow \mathbb{R}^{n}, \boldsymbol{g}(t) =  \left[ \begin{array}{c} \vdots \\ g_{*}(t) \\ \vdots \end{array} \right] \)}} \\
		{\color{red} D(} \mathbf{A} \boldsymbol{x} {\color{red})} &= {\color{black} \mathbf{A}}, \qquad &
		\fbox{\parbox{0.36\textwidth}{\( (4) \boldsymbol{g}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}, \boldsymbol{g}(\boldsymbol{x}) =  \mathbf{A} \boldsymbol{x} \)}} \\
		{\color{red} D(} \mathbf{A}(\alpha \boldsymbol{x}) {\color{red})} &= {\color{black} \alpha \mathbf{A}}, \\
		\frac{d}{d \alpha}( {\color{black}\mathbf{A}(\alpha \boldsymbol{x})} ) &= {\color{black} \mathbf{A} \boldsymbol{x}}, \\
		{\color{blue}\nabla} \boldsymbol{a}^{\top} \boldsymbol{x} &= \boldsymbol{a},  \qquad & 
		\fbox{\parbox{0.36\textwidth}{\( (2) f: \mathbb{R}^{n} \rightarrow \mathbb{R}, f(\boldsymbol{x}) =  \boldsymbol{a}^{\top} \boldsymbol{x} \)}} \\
		{\color{blue}\nabla} \mathbf{A} \boldsymbol{x} &= \mathbf{A}^{\top}, \qquad & 
		\fbox{\parbox{0.36\textwidth}{\( (4) \boldsymbol{g}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}, \boldsymbol{g}(\boldsymbol{x}) =  \mathbf{A} \boldsymbol{x} \)}} \\
		{\color{blue}\nabla} \mathbf{A}(\alpha \boldsymbol{x}) &=  \alpha \mathbf{A}^{\top}. \\
	\end{align*}
	
	\item Note that for \underline{ \(f: \mathbb{R}^n \rightarrow \mathbb{R}\) }, we have
	\[
	{\color{blue} \nabla} f(\boldsymbol{x})={\color{red} D} f(\boldsymbol{x})^{\top}.
	\]
\end{itemize}

\bigskip

%------------------------------------------------------%
\subsection{Differentiation rules on composite function}
\begin{itemize}
	\item To differentiate the composite function, \( h(t)=f(\boldsymbol{g}(t)) \) is differentiable on \((a, b)\), and
	\[
	f \left( \boldsymbol{g} \left( t \right) 	\right)=
	f \left( \left[
	\begin{array}{c}
		g_{1}( t ) \\
		g_{2}( t ) \\
		\vdots \\
		g_{m}( t )
	\end{array}
	\right]
	\right)
	= a_{1}g_{1}( t ) + a_{2}g_{2}( t ) + \cdots + a_{m}g_{m}( t ).
	\]

	\item The differentiated composite function with \textbf{derivative} rule is 
	\[
	h^{\prime}(t)=D_{\boldsymbol{g}} f(\boldsymbol{g}(t)) D_{t} \boldsymbol{g}(t)
	=\nabla f(\boldsymbol{g}(t))^{\top}
	\left[\begin{array}{c}
		g_{1}^{\prime}(t) \\
		\vdots \\
		g_{m}^{\prime}(t)
	\end{array}  \right] 
	=  \bigg[ \begin{array}{ccc}
		\frac{d}{d g_1} f(\boldsymbol{g}(t)) &
		\cdots &
		\frac{d}{d g_m} f(\boldsymbol{g}(t))
	\end{array} \bigg] \left[\begin{array}{c}
		g_{1}^{\prime}(t) \\
		\vdots \\
		g_{m}^{\prime}(t)
	\end{array}  \right]  .
	\]

%	\item The differentiated composite function with \textbf{gradient} rule is 
%	\[
%	h^{\prime}(t)= \nabla_{t} \boldsymbol{g}(t) \nabla_{\boldsymbol{g}} f(\boldsymbol{g}(t)) 
%	= \bigg[ \begin{array}{ccc}
%		g_{1}^{\prime}(t) &
%		\cdots &
%		g_{m}^{\prime}(t)
%	\end{array} \bigg]
%	\left[\begin{array}{c}
%		\frac{d}{d g_1} f(\boldsymbol{g}(t)) \\
%		\vdots \\
%		\frac{d}{d g_m} f(\boldsymbol{g}(t))
%	\end{array} \right] .
%	\]
%
%	\[
%	\boldsymbol{g} \left( \boldsymbol{x} \right)=
%	\left[
%	\begin{array}{c}
%		g_{1}( \boldsymbol{x} ) \\
%		g_{2}( \boldsymbol{x} ) \\
%		\vdots \\
%		g_{m}( \boldsymbol{x} )
%	\end{array}
%	\right]
%	\]
	
	\item Consider Hessian matrix, which is second order derivative of scalar. Noted that, \( D \left( f(\boldsymbol{x}) \right)\) is spreading the derivative of the polynomials on the horizontal direction. Thus, we would like to ensure each entry is located on a vertical direction, then the entry could be applied to conduct derivative.
	\[ D^{2} \left( f(\boldsymbol{x}) \right)
	= D \left(  D f \left( \boldsymbol{x} \right)^{\top} \right)
	= D(\nabla f(\boldsymbol{x}))
	= \left[\begin{array}{c}    
		D\left(\frac{\partial f}{ \color{forestgreen}{\partial x_{1}} }\right) \\    
		D\left(\frac{\partial f}{ \color{forestgreen}{\partial x_{2}} }\right) \\    
		\vdots \\    
		D\left(\frac{\partial f}{ \color{forestgreen}{\partial x_{n}} }\right)    
	\end{array}
	\right]
	=\left[\begin{array}{cccc}    
		\frac{\partial^{2} f}{\partial x_{1} \color{forestgreen}{\partial x_{1}}} 
		& \frac{\partial^{2} f}{\partial x_{2} \color{forestgreen}{\partial x_{1}}} 
		& \cdots 
		& \frac{\partial^{2} f}{\partial x_{n} \color{forestgreen}{\partial x_{1}}} \\    
		\frac{\partial^{2} f}{\partial x_{1} \color{forestgreen}{\partial x_{2}}} 
		& \frac{\partial^{2} f}{\partial x_{2} \color{forestgreen}{\partial x_{2}}} 
		& \cdots 
		& \frac{\partial^{2} f}{\partial x_{n} \color{forestgreen}{\partial x_{2}}} \\    
		\vdots & \vdots & \ddots & \vdots \\    
		\frac{\partial^{2} f}{\partial x_{1} \color{forestgreen}{\partial x_{n}}} 
		& \frac{\partial^{2} f}{\partial x_{2} \color{forestgreen}{\partial x_{n}}} 
		& \cdots 
		& \frac{\partial^{2} f}{\partial x_{n} \color{forestgreen}{\partial x_{n}}}
	\end{array}
	\right] .
	\]

%	\item It could be applied to conduct gradient,
%	\[ \nabla^{2} \left( f(\boldsymbol{x}) \right)
%	= \nabla \left(  \nabla f \left( \boldsymbol{x} \right) \right)
%	= \left[\begin{array}{c}    
%		\nabla \left(\frac{\partial f}{ \color{forestgreen}{\partial x_{1}} }\right) \\    
%		\nabla \left(\frac{\partial f}{ \color{forestgreen}{\partial x_{2}} }\right) \\    
%		\vdots \\    
%		\nabla \left(\frac{\partial f}{ \color{forestgreen}{\partial x_{n}} }\right)    
%	\end{array}
%	\right]
%	=\left[\begin{array}{cccc}    
%		\frac{\partial^{2} f}{\partial x_{1} \color{forestgreen}{\partial x_{1}}} 
%		& \frac{\partial^{2} f}{\partial x_{2} \color{forestgreen}{\partial x_{1}}} 
%		& \cdots 
%		& \frac{\partial^{2} f}{\partial x_{n} \color{forestgreen}{\partial x_{1}}} \\    
%		\frac{\partial^{2} f}{\partial x_{1} \color{forestgreen}{\partial x_{2}}} 
%		& \frac{\partial^{2} f}{\partial x_{2} \color{forestgreen}{\partial x_{2}}} 
%		& \cdots 
%		& \frac{\partial^{2} f}{\partial x_{n} \color{forestgreen}{\partial x_{2}}} \\    
%		\vdots & \vdots & \ddots & \vdots \\    
%		\frac{\partial^{2} f}{\partial x_{1} \color{forestgreen}{\partial x_{n}}} 
%		& \frac{\partial^{2} f}{\partial x_{2} \color{forestgreen}{\partial x_{n}}} 
%		& \cdots 
%		& \frac{\partial^{2} f}{\partial x_{n} \color{forestgreen}{\partial x_{n}}}
%	\end{array}
%	\right] .
%	\]
\end{itemize}


%------------------------------------------------------%
\subsection{Differentiation Product Rules}

\textbf{i)} Let \(f: \mathbb{R} \rightarrow \mathbb{R}\) and \(g: \mathbb{R} \rightarrow \mathbb{R}\) be two differentiable functions, \(x \in \mathbb{R}\),
\[ 
\begin{aligned}
	D \bigg (f(x) g(x) \bigg )
	& = f(x)  D g(x)
	+g(x)  D f(x),  \\
	\nabla \bigg (f(x) g(x) \bigg )
	& = f(x)  \nabla g(x)
	+g(x)  \nabla f(x). 
\end{aligned}
\]

\noindent
\textbf{ii)} Let \(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\) and \(g: \mathbb{R}^{n} \rightarrow \mathbb{R}\) be two differentiable functions, \(\boldsymbol{x} \in \mathbb{R}^{n}\),
\[ 
\begin{aligned}
	D \bigg (f(\boldsymbol{x}) g(\boldsymbol{x}) \bigg )
	& = f(\boldsymbol{x}) \big [\begin{array}{ccc}
		&D g(\boldsymbol{x}) & \end{array} \big ] 
	+g(\boldsymbol{x}) \big [\begin{array}{ccc} & D f(\boldsymbol{x}) &
	\end{array} \big ], \\
	\nabla \bigg (f(\boldsymbol{x}) g(\boldsymbol{x}) \bigg )
	& = f(\boldsymbol{x}) \left [\begin{array}{c}
		\\ \nabla g(\boldsymbol{x}) \\  \\ \end{array} \right ] 
	+g(\boldsymbol{x}) \left [\begin{array}{c} \\ \nabla f(\boldsymbol{x}) \\ \\
	\end{array} \right ].
\end{aligned}
\]

\noindent
\textbf{iii)} Let \( \boldsymbol{f}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\) and \( \boldsymbol{g}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}\) be two differentiable functions, \(\boldsymbol{x} \in \mathbb{R}^{n}\),
\[ 
\begin{aligned}
	D \bigg (\boldsymbol{f}(\boldsymbol{x})^{\top} \boldsymbol{g}(\boldsymbol{x}) \bigg )
	& =\boldsymbol{f}(\boldsymbol{x})^{\top} D \boldsymbol{g}(\boldsymbol{x})+\boldsymbol{g}(\boldsymbol{x})^{\top} D \boldsymbol{f}(\boldsymbol{x}), \\
	\nabla \bigg (\boldsymbol{f}(\boldsymbol{x})^{\top} \boldsymbol{g}(\boldsymbol{x}) \bigg )
	& =\boldsymbol{f}(\boldsymbol{x})^{\top} \nabla \boldsymbol{g}(\boldsymbol{x})+\boldsymbol{g}(\boldsymbol{x})^{\top} \nabla \boldsymbol{f}(\boldsymbol{x}). \\
\end{aligned}
\]

\begin{itemize}
	\item Based on the above \textbf{derivative} rule, we have

	\textbf{1.} Consider \(\boldsymbol{A} \in \mathbb{R}^{m \times n}\) be a given matrix and \(\boldsymbol{y} \in \mathbb{R}^{m}\) a given vector. Then,
	\begin{align*}
		D\left(\boldsymbol{y}^{\top} \boldsymbol{A} \boldsymbol{x}\right) &=
		\boldsymbol{y}^{\top} \boldsymbol{A}, \\
		D\left(\boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x}\right) &=
		\boldsymbol{x}^{\top}\left(\boldsymbol{A}+\boldsymbol{A}^{\top}\right).  \qquad
		\fbox{\parbox{0.1\textwidth}{ if \(m=n\) }}
	\end{align*}
	
	
	\textbf{2.} Consider \(\boldsymbol{A} \in \mathbb{R}^{m \times n}\) be a given matrix and \(\boldsymbol{y} \in \mathbb{R}^{n}\) a given vector. Then,
	\[ D\left(\boldsymbol{y}^{\top} \boldsymbol{x}\right)=\boldsymbol{y}^{\top} \]
	
	\textbf{3.} Consider if \(\boldsymbol{Q}\) is a symmetric matrix, then
	\[
	D\left(\boldsymbol{x}^{\top} \boldsymbol{Q} \boldsymbol{x}\right)=2 \boldsymbol{x}^{\top} \boldsymbol{Q}
	\]
	
	In particular,
	\[
	D\left(\boldsymbol{x}^{\top} \boldsymbol{x}\right)=2 \boldsymbol{x}^{\top}
	\]

	\item Based on the above \textbf{gradient} rule, we have

	\textbf{1.} Consider \(\boldsymbol{A} \in \mathbb{R}^{m \times n}\) be a given matrix and \(\boldsymbol{y} \in \mathbb{R}^{m}\) a given vector. Then,
	\begin{align*}
		\nabla \left(\boldsymbol{y}^{\top} \boldsymbol{A} \boldsymbol{x}\right) &= \boldsymbol{A}^{\top} \boldsymbol{y}, \\
		\nabla  \left(\boldsymbol{x}^{\top} \boldsymbol{A} \boldsymbol{x}\right) &=
		\left(\boldsymbol{A}+\boldsymbol{A}^{\top}\right) \boldsymbol{x}.  \qquad
		\fbox{\parbox{0.1\textwidth}{ if \(m=n\) }}
	\end{align*}
	
	
	\textbf{2.} Consider \(\boldsymbol{A} \in \mathbb{R}^{m \times n}\) be a given matrix and \(\boldsymbol{y} \in \mathbb{R}^{n}\) a given vector. Then,
	\[ \nabla \left(\boldsymbol{y}^{\top} \boldsymbol{x}\right)=\boldsymbol{y} \]
	
	\textbf{3.} Consider if \(\boldsymbol{Q}\) is a symmetric matrix, then
	\[
	\nabla \left(\boldsymbol{x}^{\top} \boldsymbol{Q} \boldsymbol{x}\right)=2  \boldsymbol{Q} \boldsymbol{x}
	\]
	
	In particular,
	\[
	\nabla \left(\boldsymbol{x}^{\top} \boldsymbol{x}\right)=2 \boldsymbol{x}
	\]

\end{itemize}

\bigskip

\noindent

[Ref]: 

Edwin K.P. Chong, Stanislaw H. Żak, ``PART I MATHEMATICAL REVIEW" in ``An introduction to optimization", 4th Edition, John Wiley and Sons, Inc. 2013.


\end{document}
