%------------------------------------------------------%
%------------------------------------------------------%
\section{Minimize \(\|\boldsymbol{x}\|\) subject to \(\boldsymbol{A} \boldsymbol{x}=\boldsymbol{b}\)}
%------------------------------------------------------%
%------------------------------------------------------%

%------------------------------------------------------%
\subsection{Concept}
\begin{itemize}
	\item Theorem  Consider a system of linear equations \(\boldsymbol{A} \boldsymbol{x}=\boldsymbol{b}\), \(\boldsymbol{A} \in \mathbb{R}^{m \times n}\), \(\operatorname{rank}(\boldsymbol{A})=\) \(r\). The vector \(\boldsymbol{x}^{*}=\boldsymbol{A}^{\dagger} \boldsymbol{b}\) minimizes \(\|\boldsymbol{A} \boldsymbol{x}-\boldsymbol{b}\|^{2}\) on \(\mathbb{R}^{n}\). Furthermore, among all vectors in \(\mathbb{R}^{n}\) that minimizes \(\|\boldsymbol{A} \boldsymbol{x}-\boldsymbol{b}\|^{2}\), the vector \(\boldsymbol{x}^{*}=\boldsymbol{A}^{\dagger} \boldsymbol{b}\) is the unique vector with minimal norm.
\end{itemize}

%------------------------------------------------------%
\subsection{Examples}
Example 1: Consider a system of linear equations, \(\boldsymbol{A} \boldsymbol{x}=\boldsymbol{b}\), where

\[
	\boldsymbol{A}=\left[\begin{array}{lll}
		0 & 1 & 1 \\
		0 & 2 & 2
	\end{array}\right], b=\left[\begin{array}{l}
		2 \\
		1
	\end{array}\right] .
\]

Find the minimum length vector \(\boldsymbol{x}^{*}\) that minimizes \(\|\boldsymbol{A} \boldsymbol{x}-\boldsymbol{B}\|_{2}^{2}\).

Solution:

The minimium length vector \(\boldsymbol{x}^{*}\) that minimizes \(\|\boldsymbol{A} \boldsymbol{x}-\boldsymbol{b}\|_{2}^{2}\) is

\[
	\boldsymbol{x}^{*}=\boldsymbol{A}^{\dagger} \boldsymbol{b},
\]

where

\[
	\boldsymbol{A}^{\dagger}=\boldsymbol{C}^{\dagger} \boldsymbol{B}^{\dagger},
\]

and the matrices \(\boldsymbol{B}\) and \(\boldsymbol{C}\) are satisfying

\[
	\boldsymbol{A}=\boldsymbol{B} \boldsymbol{C}, \operatorname{rank}(\boldsymbol{A})=\operatorname{rank}(\boldsymbol{B})=\operatorname{rank}(\boldsymbol{C})=1 .
\]

We have

\[
	\boldsymbol{A}=\boldsymbol{B} \boldsymbol{C}=\left[\begin{array}{l}
		1 \\
		2
	\end{array}\right]\left[\begin{array}{lll}
		0 & 1 & 1
	\end{array}\right].
\]

Hence

\[
	\boldsymbol{B}^{\dagger}=\left(\boldsymbol{B}^{\top} \boldsymbol{B}\right)^{-1} \boldsymbol{B}^{\top}=\left(\left[\begin{array}{ll}
		1 & 2
	\end{array}\right]\left[\begin{array}{l}
		1 \\
		2
	\end{array}\right]\right)^{-1}\left[\begin{array}{ll}
		1 & 2
	\end{array}\right]=\frac{1}{5}\left[\begin{array}{ll}
		1 & 2
	\end{array}\right]
\]

and

\[
	\boldsymbol{C}^{\dagger}=\boldsymbol{C}^{\top}\left(\boldsymbol{C} \boldsymbol{C}^{\top}\right)^{-1}=\frac{1}{2}\left[\begin{array}{l}
		0 \\
		1 \\
		1
	\end{array}\right].
\]

Therefore,

\[
	\boldsymbol{A}^{\dagger}=\frac{1}{10}\left[\begin{array}{ll}
		0 & 0 \\
		1 & 2 \\
		1 & 2
	\end{array}\right]
\]

and

\[
	\boldsymbol{x}^{*}=\boldsymbol{A}^{\dagger} \boldsymbol{b}=\left[\begin{array}{c}
		0 \\
		0.4 \\
		0.4
	\end{array}\right] .
\]

Example 2:

\[
	\begin{array}{rl}
		\min & \|\boldsymbol{x}\|_{2} \\
		& \\
		\text { subject to} & {\left[\begin{array}{ll}
				a & b \\
				a & b \\
				a & b
			\end{array}\right] \boldsymbol{x}=\left[\begin{array}{l}
				1 \\
				1 \\
				1
			\end{array}\right]. }
	\end{array}
\]

where \(a\) and \(b\) are non-zero real parameters.

\textbf{Short answer}:

We have \(\boldsymbol{x}^{*}=\boldsymbol{A}^{\dagger} \boldsymbol{b}\), where

\[
	\boldsymbol{A}=\boldsymbol{B} \boldsymbol{C}=\left[\begin{array}{l}
		1 \\
		1 \\
		1
	\end{array}\right]\left[\begin{array}{ll}
		a & b
	\end{array}\right] .
\]

Thus

\[
	\boldsymbol{A}^{\dagger}=\boldsymbol{C}^{\dagger} \boldsymbol{B}^{\dagger}=\boldsymbol{C}^{\top}\left(\boldsymbol{C} \boldsymbol{C}^{\top}\right)^{-1}\left(\boldsymbol{B}^{\top} \boldsymbol{B}\right)^{-1} \boldsymbol{B}^{\top}=\frac{1}{3\left(a^{2}+b^{2}\right)}\left[\begin{array}{ccc}
		a & a & a \\
		b & b & b
	\end{array}\right] .
\]

Therefore,

\[
	\boldsymbol{x}^{*}=\frac{1}{3\left(a^{2}+b^{2}\right)}\left[\begin{array}{lll}
		a & a & a \\
		b & b & b
	\end{array}\right]\left[\begin{array}{l}
		1 \\
		1 \\
		1
	\end{array}\right]=\frac{1}{a^{2}+b^{2}}\left[\begin{array}{l}
		a \\
		b
	\end{array}\right] .
\]

Example 3: Let \(\boldsymbol{A} \in \mathbb{R}^{m \times n}, b \in \mathbb{R}^{m}, m \geq n\), and \(\operatorname{rank}(\boldsymbol{A})=n\). Consider the constrained optimization problem

\[
	\begin{array}{rl}
		\operatorname{minimize} & \dfrac{1}{2} \boldsymbol{x}^{\top} \boldsymbol{x}-\boldsymbol{x}^{\top} \boldsymbol{b} \\
		& \\
		\text { subject to} & x \in \mathcal{R}(\boldsymbol{A})
	\end{array}
\]

where \(\mathcal{R}(\boldsymbol{A})\) denotes the range of \(\boldsymbol{A}\). Derive an expression for the global minimizer of this problem in terms of \(\boldsymbol{A}\) and \(\boldsymbol{b}\).

\textbf{Short answer}:

The optimization problem can be rewritten as

\[
	\begin{array}{rl}
		\operatorname{minimize} & \dfrac{1}{2}\|\boldsymbol{x}-\boldsymbol{b}\|^{2} \\
		& \\
		\text { subject to} & x \in \mathcal{R}(\boldsymbol{A}) .
	\end{array}
\]

Substituting \(\boldsymbol{x}=\boldsymbol{A} \boldsymbol{y}\), we see that this is simply a linear least squares problem with decision variable \(\boldsymbol{y}\). The solution to the least squares problem is

\[
	\boldsymbol{y}^{*}=\left(\boldsymbol{A}^{\top} \boldsymbol{A}\right)^{-1} \boldsymbol{A}^{\top} \boldsymbol{b},
\]

which implies that the solution to the given problem is

\[
	\boldsymbol{x}^{*}=\boldsymbol{A} \boldsymbol{y}^{*}=\boldsymbol{A}\left(\boldsymbol{A}^{\top} \boldsymbol{A}\right)^{-1} \boldsymbol{A}^{\top} \boldsymbol{b}.
\]

Example 4: Solve the problem

\[
	\begin{array}{rl}
		\operatorname{ minimize} & \dfrac{1}{2}\left\|\boldsymbol{x}-\boldsymbol{x}_{0}\right\| \\
		& \\
		\text { subject to} & {\left[\begin{array}{lll}
				1 & 1 & 1
			\end{array}\right] \boldsymbol{x}=1 }.
	\end{array}
\]

where \(x_{0}=\left[\begin{array}{lll}0 & -3 & 0\end{array}\right]\).

\textbf{Short answer}:

Let \(\boldsymbol{z}=\boldsymbol{x}-\boldsymbol{x}_{0}, \boldsymbol{A}=\left[\begin{array}{lll}1 & 1 & 1\end{array}\right]\), and \(\boldsymbol{b}=1\). We then have

\[
	\begin{array}{rl}
		\operatorname{minimize} & \dfrac{1}{2}\|\boldsymbol{z}\| \\
		& \\
		\text { subject to} & \boldsymbol{A} \boldsymbol{z}=\boldsymbol{b}-\boldsymbol{A} \boldsymbol{x}_{0} .
	\end{array}
\]

The solution to the above problem has the form

\[
	\begin{aligned}
		\boldsymbol{z}^{*} & =\boldsymbol{A}^{\top}\left(\boldsymbol{A} \boldsymbol{A}^{\top}\right)^{-1}\left(\boldsymbol{b}-\boldsymbol{A} \boldsymbol{x}_{0}\right) \\
		& =\boldsymbol{A}^{\top}\left(\boldsymbol{A} \boldsymbol{A}^{\top}\right)^{-1} \boldsymbol{b}-\boldsymbol{A}^{\top}\left(\boldsymbol{A} \boldsymbol{A}^{\top}\right)^{-1} \boldsymbol{A} \boldsymbol{x}_{0} .
	\end{aligned}
\]

Then we have

\[
	\begin{aligned}
		\boldsymbol{x}^{*} & =\boldsymbol{z}^{*}+\boldsymbol{x}_{0} \\
		& =\boldsymbol{A}^{\top}\left(\boldsymbol{A} \boldsymbol{A}^{\top}\right)^{-1} \boldsymbol{b}+\left(\boldsymbol{I}_{n}-\boldsymbol{A}^{\top}\left(\boldsymbol{A} \boldsymbol{A}^{\top}\right)^{-1} \boldsymbol{A}\right) \boldsymbol{x}_{0} .
	\end{aligned}
\]

Substituting the parameters into the formula above, we obtain

\[
	\boldsymbol{x}^{*}=\left[\begin{array}{r}
		\frac{4}{3} \vspace{2mm}\\ 
		-\frac{5}{3} \vspace{2mm}\\
		\frac{4}{3}
	\end{array}\right] .
\]

Example 5: The Ridge Regression problem is a variant of least squares:

\[
	\text { minimize }\|\boldsymbol{b}-\boldsymbol{A} \boldsymbol{x}\|_{2}^{2}+\lambda\|\boldsymbol{x}\|_{2}^{2} .
\]

Show that this problem always has a unique solution, for any \(\boldsymbol{A}\) if \(\lambda>0\).

\textbf{Short answer}:

Let \(f(\boldsymbol{x})=\|\boldsymbol{b}-\boldsymbol{A} \boldsymbol{x}\|_{2}^{2}+\lambda\|\boldsymbol{x}\|_{2}^{2}\), then we have

\[
	\begin{aligned}
		f(\boldsymbol{x}) & =(\boldsymbol{b}-\boldsymbol{A} \boldsymbol{x})^{\top}(\boldsymbol{b}-\boldsymbol{A} \boldsymbol{x})+\lambda \boldsymbol{x}^{\top} \boldsymbol{x} \\
		& =\boldsymbol{x}^{\top}\left(\boldsymbol{A}^{\top} \boldsymbol{A}+\lambda \boldsymbol{I}\right) \boldsymbol{x}-2 \boldsymbol{b}^{\top} \boldsymbol{A} \boldsymbol{x} .
	\end{aligned}
\]

Since this is an unconstrained problem, we first use the first-order necessary condition to get the candidate point. \(\nabla f(\boldsymbol{x})=0\) implies

\[
	\left(\boldsymbol{A}^{\top} \boldsymbol{A}+\lambda \boldsymbol{I}\right) \boldsymbol{x}=\boldsymbol{A}^{\top} \boldsymbol{b}.
\]

Since \(\boldsymbol{A}^{\top} \boldsymbol{A}\) is real symmetric positive semi-definite and \(\lambda>0\), we have \(\boldsymbol{A}^{\top} \boldsymbol{A}+\lambda \boldsymbol{I} \succ 0\). Therefore, \(\operatorname{eig}\left(\boldsymbol{A}^{\top} \boldsymbol{A}+\lambda \boldsymbol{I} \right)>0\). Matrix \(\boldsymbol{A}^{\top} \boldsymbol{A}+\lambda \boldsymbol{I}\) is non-singular, \(\left(\boldsymbol{A}^{\top} \boldsymbol{A}+\lambda \boldsymbol{I}\right)^{-1}\) exists. The only candidate point we find here is

\[
	\boldsymbol{x}^{*}=\left(\boldsymbol{A}^{\top} \boldsymbol{A}+\lambda \boldsymbol{I} \right)^{-1} \boldsymbol{A}^{\top} \boldsymbol{b} .
\]

We now use the second-order sufficient condition. The Hessian \(\boldsymbol{F} \left(\boldsymbol{x}^{*}\right)=\boldsymbol{A}^{\top} \boldsymbol{A}+ \lambda I \succ 0\). This implies the only solution we find is a strict local minimizer. Therefore, \(\boldsymbol{x}^{*}\) is the unique solution to this problem.
